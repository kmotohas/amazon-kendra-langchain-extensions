{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96891362-46d6-4d85-9631-d64f50e5194f",
   "metadata": {},
   "source": [
    "# RAG with Kendra\n",
    "\n",
    "#### If you are running this in sagemaker studio you need to select a kernel with a python version >3.8\n",
    "\n",
    "このノートブックでは、rinna 社の japanese-gpt-neox-3.6b-instruction-ppo モデルを、SageMaker のリアルタイム推論エンドポイントとして Hosting し、Amazon Kendra で取得したドキュメントを要約して返す RAG (Retrieval Augmented Generation) を体験します。\n",
    "以下の環境で動作確認を行なっています。\n",
    "\n",
    "- SageMaker Studio Notebooks\n",
    "  - `ml.m5.2xlarge (RAM 32GB, vCPU 8)` : `PyTorch 1.13 Python 3.9 CPU Optimized`\n",
    "  \n",
    "前半の rinna をデプロイするパートは [aws-ml-jp で公開しているノートブック](https://github.com/aws-samples/aws-ml-jp/blob/main/tasks/generative-ai/text-to-text/inference/deploy-endpoint/Transformers/rinna-3.6b-instruction-ppo_Inference.ipynb.ipynb)の手順に基づいています。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fa9b97-9abe-43bd-8868-7d90b51a3c2e",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "This notebook is based on the this aws blog \n",
    "* https://aws.amazon.com/blogs/machine-learning/quickly-build-high-accuracy-generative-ai-applications-on-enterprise-data-using-amazon-kendra-langchain-and-large-language-models/\n",
    "\n",
    "---\n",
    "\n",
    "### Solution overview\n",
    "The following diagram shows the architecture of a GenAI application with a RAG approach.\n",
    "\n",
    "<img src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2023/05/02/ML-13807-image001-new.png\">\n",
    "\n",
    "We use an Amazon Kendra index to ingest enterprise unstructured data from data sources such as wiki pages, MS SharePoint sites, Atlassian Confluence, and document repositories such as Amazon S3. When a user interacts with the GenAI app, the flow is as follows:\n",
    "\n",
    "1. The user makes a request to the GenAI app.\n",
    "2. The app issues a search query to the Amazon Kendra index based on the user request.\n",
    "3. The index returns search results with excerpts of relevant documents from the ingested enterprise data.\n",
    "4. The app sends the user request and along with the data retrieved from the index as context in the LLM prompt.\n",
    "5. The LLM returns a succinct response to the user request based on the retrieved data.\n",
    "6. The response from the LLM is sent back to the user.\n",
    "\n",
    "With this architecture, you can choose the most suitable LLM for your use case. LLM options include our partners Hugging Face, AI21 Labs, Cohere, and others hosted on an Amazon SageMaker endpoint, as well as models by companies like Anthropic and OpenAI. With Amazon Bedrock, you will be able to choose Amazon Titan, Amazon’s own LLM, or partner LLMs such as those from AI21 Labs and Anthropic with APIs securely without the need for your data to leave the AWS ecosystem. The additional benefits that Amazon Bedrock will offer include a serverless architecture, a single API to call the supported LLMs, and a managed service to streamline the developer workflow.\n",
    "\n",
    "For the best results, a GenAI app needs to engineer the prompt based on the user request and the specific LLM being used. Conversational AI apps also need to manage the chat history and the context. GenAI app developers can use open-source frameworks such as LangChain that provide modules to integrate with the LLM of choice, and orchestration tools for activities such as chat history management and prompt engineering. We have provided the KendraIndexRetriever class, which implements a LangChain retriever interface, which applications can use in conjunction with other LangChain interfaces such as chains to retrieve data from an Amazon Kendra index. We have also provided a few sample applications in the GitHub repo. You can deploy this solution in your AWS account using the step-by-step guide in this post."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a935567-2d68-4ba5-887e-a8e15fbad714",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "0. [Prerequisites](#Prerequisites)\n",
    "1. [Permissions and environment variables](#1.-Permissions-and-environment-variables)\n",
    "2. [Select a pre-trained model](#2.-Select-a-pre-trained-model)\n",
    "3. [Retrieve Artifacts & Deploy an Endpoint](#3.-Retrieve-Artifacts-&-Deploy-an-Endpoint)\n",
    "4. [Query endpoint and parse response](#4.-Query-endpoint-and-parse-response)\n",
    "5. [Query endpoint with Langchain and Kendra Index](#5.-Query-endpoint-with-Langchain-and-Kendra-Index)\n",
    "6. [[OPTIONAL] Installing Streamlet application and running a WebUI for a chatbot](#6.-[OPTIONAL]-Installing-Streamlit-application-and-running-a-WebUI-for-a-chatbot)\n",
    "7. [Clean up the endpoint](#7.-Clean-up-the-endpoint)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7657a8cc-a1b3-4328-9606-9d62290715af",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "### Kendra Index\n",
    "\n",
    "---\n",
    "\n",
    "Use the provided AWS CloudFormation to create a new Amazon Kendra index. This template includes sample data containing AWS online documentation for Amazon Kendra, Amazon Lex, and Amazon SageMaker. Alternately, if you have an Amazon Kendra index and have indexed your own dataset, you can use that. \n",
    "\n",
    "\n",
    "Deployment steps:\n",
    "   1. Download the [template](https://github.com/kmotohas/amazon-kendra-langchain-extensions/blob/main/kendra_retriever_samples/kendra-docs-index.yaml) from the github repo\n",
    "   2. Deploy it using the [cloudformation console](https://us-east-1.console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/create)\n",
    "      1. Select Upload a template file and then the choose file button to select the template you just downloaded\n",
    "      2. Press Next\n",
    "   3. Provide a stack name and press Next   \n",
    "   4. Leave all default options and press Next\n",
    "   5. Check the acknowledgement at the bottom and press Submit\n",
    "   6. The stack will take around 15 minutes to deploy\n",
    "   7. Take note of the `AWSRegion` and `KendraIndexID` in the Outputs tab as we will need it in later steps   \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2adb33-8b75-4769-b292-90b04d051374",
   "metadata": {},
   "source": [
    "### Kendra Permissions\n",
    "---\n",
    "\n",
    "You will need to update your SageMaker execution role with permissions to query Kendra. \n",
    "\n",
    "1. Navigate to IAM console and select Roles\n",
    "2. Search for your SageMaker execution role it will look like `AmazonSageMaker-ExecutionRole-<TIMESTAMP>`\n",
    "3. Select your role and add permission search for `AmazonKendraReadOnlyAccess` and attach the policy\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2d5df5-6684-4e33-b4a5-7015c11d784f",
   "metadata": {},
   "source": [
    "### AWS Langchain\n",
    "\n",
    "---\n",
    "\n",
    "This repo provides a set of utility classes to work with Langchain. It currently has a retriever class KendraIndexRetriever for working with a Kendra index and sample scripts to execute the QA chain for SageMaker, Open AI and Anthropic providers.\n",
    "\n",
    "---\n",
    "\n",
    "Clone the repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "656a95fc-7a8f-4f44-b034-4c2ad2160974",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'amazon-kendra-langchain-extensions'...\n",
      "remote: Enumerating objects: 75, done.\u001b[K\n",
      "remote: Counting objects: 100% (56/56), done.\u001b[K\n",
      "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
      "remote: Total 75 (delta 38), reused 22 (delta 13), pack-reused 19\u001b[K\n",
      "Receiving objects: 100% (75/75), 25.33 KiB | 741.00 KiB/s, done.\n",
      "Resolving deltas: 100% (39/39), done.\n"
     ]
    }
   ],
   "source": [
    "#!git clone https://github.com/aws-samples/amazon-kendra-langchain-extensions.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60cd17c-c417-44f1-bccf-c2941628d801",
   "metadata": {
    "tags": []
   },
   "source": [
    "Install the classes:\n",
    "we use a specific version of amazon-kendra-langchain-extensions as it later gets integrated into langchain and would require change of the code of this lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2de88b05-3a2d-4613-87a9-5a96c8bbb326",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: switching to '28cb1d4de7cf3bfe8984c6365ce248c12e8b77e0'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by switching back to a branch.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -c with the switch command. Example:\n",
      "\n",
      "  git switch -c <new-branch-name>\n",
      "\n",
      "Or undo this operation with:\n",
      "\n",
      "  git switch -\n",
      "\n",
      "Turn off this advice by setting config variable advice.detachedHead to false\n",
      "\n",
      "HEAD is now at 28cb1d4 Don't take a source if it is already present. Fixed hardcoding of region\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "distributed 2022.7.0 requires tornado<6.2,>=6.0.3, but you have tornado 6.3.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#!cd amazon-kendra-langchain-extensions && git checkout 28cb1d4de7cf3bfe8984c6365ce248c12e8b77e0 && pip install . --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740a58ac-3980-40a3-ab49-3a884b33c9ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. 準備\n",
    "\n",
    "### ノートブックを動かすに当たって必要なモジュールのインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "327d1122-fa99-4a77-b8cc-4fdbcdb7125a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "35e3d765-8278-4210-acac-f588044b6ac5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.9/site-packages (23.1.2)\n",
      "Requirement already satisfied: sagemaker in /opt/conda/lib/python3.9/site-packages (2.171.0)\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.9/site-packages (1.28.1)\n",
      "Requirement already satisfied: transformers==4.26 in /opt/conda/lib/python3.9/site-packages (4.26.0)\n",
      "Requirement already satisfied: einops in /opt/conda/lib/python3.9/site-packages (0.6.1)\n",
      "Requirement already satisfied: SentencePiece in /opt/conda/lib/python3.9/site-packages (0.1.99)\n",
      "Requirement already satisfied: langchain in /opt/conda/lib/python3.9/site-packages (0.0.229)\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.9/site-packages (8.0.7)\n",
      "Requirement already satisfied: pydantic<2 in /opt/conda/lib/python3.9/site-packages (1.10.11)\n",
      "Collecting streamlit\n",
      "  Using cached streamlit-1.24.1-py2.py3-none-any.whl (8.9 MB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.26) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.26) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.26) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.26) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.26) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.26) (2023.6.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.26) (2.28.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.26) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers==4.26) (4.64.1)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.9/site-packages (from sagemaker) (23.1.0)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.9/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.9/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /opt/conda/lib/python3.9/site-packages (from sagemaker) (3.20.2)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.9/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /opt/conda/lib/python3.9/site-packages (from sagemaker) (4.13.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from sagemaker) (1.5.3)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.9/site-packages (from sagemaker) (0.3.0)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.9/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.9/site-packages (from sagemaker) (4.18.0)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.9/site-packages (from sagemaker) (3.8.1)\n",
      "Requirement already satisfied: tblib==1.7.0 in /opt/conda/lib/python3.9/site-packages (from sagemaker) (1.7.0)\n",
      "Requirement already satisfied: botocore<1.32.0,>=1.31.1 in /opt/conda/lib/python3.9/site-packages (from boto3) (1.31.1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.9/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.9/site-packages (from boto3) (0.6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.9/site-packages (from langchain) (2.0.18)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.9/site-packages (from langchain) (3.8.4)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.9/site-packages (from langchain) (4.0.2)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /opt/conda/lib/python3.9/site-packages (from langchain) (0.5.9)\n",
      "Requirement already satisfied: langchainplus-sdk<0.0.21,>=0.0.20 in /opt/conda/lib/python3.9/site-packages (from langchain) (0.0.20)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /opt/conda/lib/python3.9/site-packages (from langchain) (2.8.4)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /opt/conda/lib/python3.9/site-packages (from langchain) (1.2.4)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.9/site-packages (from langchain) (8.2.1)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.9/site-packages (from ipywidgets) (5.5.6)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.9/site-packages (from ipywidgets) (7.16.3)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.9/site-packages (from ipywidgets) (4.3.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.7 in /opt/conda/lib/python3.9/site-packages (from ipywidgets) (4.0.8)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.7 in /opt/conda/lib/python3.9/site-packages (from ipywidgets) (3.0.8)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.9/site-packages (from pydantic<2) (4.4.0)\n",
      "Collecting altair<6,>=4.0 (from streamlit)\n",
      "  Using cached altair-5.0.1-py3-none-any.whl (471 kB)\n",
      "Collecting blinker<2,>=1.0.0 (from streamlit)\n",
      "  Using cached blinker-1.6.2-py3-none-any.whl (13 kB)\n",
      "Collecting cachetools<6,>=4.0 (from streamlit)\n",
      "  Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: click<9,>=7.0 in /opt/conda/lib/python3.9/site-packages (from streamlit) (8.1.2)\n",
      "Requirement already satisfied: pillow<10,>=6.2.0 in /opt/conda/lib/python3.9/site-packages (from streamlit) (9.4.0)\n",
      "Requirement already satisfied: pyarrow>=4.0 in /opt/conda/lib/python3.9/site-packages (from streamlit) (11.0.0)\n",
      "Collecting pympler<2,>=0.9 (from streamlit)\n",
      "  Using cached Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
      "Requirement already satisfied: python-dateutil<3,>=2 in /opt/conda/lib/python3.9/site-packages (from streamlit) (2.8.2)\n",
      "Requirement already satisfied: rich<14,>=10.11.0 in /opt/conda/lib/python3.9/site-packages (from streamlit) (12.6.0)\n",
      "Requirement already satisfied: toml<2 in /opt/conda/lib/python3.9/site-packages (from streamlit) (0.10.2)\n",
      "Collecting tzlocal<5,>=1.1 (from streamlit)\n",
      "  Using cached tzlocal-4.3.1-py3-none-any.whl (20 kB)\n",
      "Collecting validators<1,>=0.2 (from streamlit)\n",
      "  Using cached validators-0.20.0-py3-none-any.whl\n",
      "Collecting gitpython!=3.1.19,<4,>=3 (from streamlit)\n",
      "  Using cached GitPython-3.1.32-py3-none-any.whl (188 kB)\n",
      "Collecting pydeck<1,>=0.1.dev5 (from streamlit)\n",
      "  Using cached pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /opt/conda/lib/python3.9/site-packages (from streamlit) (6.0.4)\n",
      "Collecting watchdog (from streamlit)\n",
      "  Using cached watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.1.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.9/site-packages (from altair<6,>=4.0->streamlit) (3.1.2)\n",
      "Requirement already satisfied: toolz in /opt/conda/lib/python3.9/site-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.9/site-packages (from botocore<1.32.0,>=1.31.1->boto3) (1.26.14)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /opt/conda/lib/python3.9/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /opt/conda/lib/python3.9/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /opt/conda/lib/python3.9/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3->streamlit)\n",
      "  Using cached gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.26) (2023.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.13.0)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: jupyter-client in /opt/conda/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.5)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (59.3.0)\n",
      "Requirement already satisfied: jedi<=0.17.2,>=0.10 in /opt/conda/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.17.2)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.36)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (2.14.0)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pexpect in /opt/conda/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.9/site-packages (from jsonschema->sagemaker) (2023.6.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.9/site-packages (from jsonschema->sagemaker) (0.29.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.9/site-packages (from jsonschema->sagemaker) (0.8.10)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->sagemaker) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil<3,>=2->streamlit) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.26) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.26) (2022.12.7)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /opt/conda/lib/python3.9/site-packages (from rich<14,>=10.11.0->streamlit) (0.9.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.9/site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
      "Collecting pytz-deprecation-shim (from tzlocal<5,>=1.1->streamlit)\n",
      "  Using cached pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /opt/conda/lib/python3.9/site-packages (from pathos->sagemaker) (1.7.6.6)\n",
      "Requirement already satisfied: dill>=0.3.6 in /opt/conda/lib/python3.9/site-packages (from pathos->sagemaker) (0.3.6)\n",
      "Requirement already satisfied: pox>=0.3.2 in /opt/conda/lib/python3.9/site-packages (from pathos->sagemaker) (0.3.2)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /opt/conda/lib/python3.9/site-packages (from pathos->sagemaker) (0.70.14)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.9/site-packages (from schema->sagemaker) (21.6.0)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3->streamlit)\n",
      "  Using cached smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from jedi<=0.17.2,>=0.10->ipython>=6.1.0->ipywidgets) (0.7.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.2)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.9/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in /opt/conda/lib/python3.9/site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (4.9.2)\n",
      "Requirement already satisfied: pyzmq>=13 in /opt/conda/lib/python3.9/site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (24.0.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.9/site-packages (from pexpect->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Collecting tzdata (from pytz-deprecation-shim->tzlocal<5,>=1.1->streamlit)\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Installing collected packages: watchdog, validators, tzdata, smmap, pympler, cachetools, blinker, pytz-deprecation-shim, pydeck, gitdb, tzlocal, gitpython, altair, streamlit\n",
      "Successfully installed altair-5.0.1 blinker-1.6.2 cachetools-5.3.1 gitdb-4.0.10 gitpython-3.1.32 pydeck-0.8.1b0 pympler-1.0.1 pytz-deprecation-shim-0.1.0.post0 smmap-5.0.0 streamlit-1.24.1 tzdata-2023.3 tzlocal-4.3.1 validators-0.20.0 watchdog-3.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U pip sagemaker boto3 transformers==4.26 einops SentencePiece langchain ipywidgets \"pydantic<2\" streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0848ea-93fb-4e7f-9f9b-87120bf0929d",
   "metadata": {},
   "source": [
    "### 今回扱うモデルの動かし方について\n",
    "[How to use the model](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-ppo#how-to-use-the-model) に沿って実行すると動かせます。\n",
    "例えば、以下のコードをこのノートブックで実行するとテキストを生成できます。\n",
    "実行したい場合は別途セルを用意して実行してみてください。g5.2xlarge インスタンスで実行に 10 分程度かかります。(ほとんどはモデルのロード時間です)\n",
    "このノートブックでは以下のコードをベースに SageMaker で Hosting できるようにします。\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'rinna/japanese-gpt-neox-3.6b-instruction-ppo', \n",
    "    use_fast=False\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'rinna/japanese-gpt-neox-3.6b-instruction-ppo'\n",
    ").to(\"cuda\")\n",
    "\n",
    "prompt = '''ユーザー: 世界自然遺産を列挙してください。\n",
    "システム: 膨大な数です。例えば国で絞ってください。\n",
    "ユーザー: イギリスでお願いします。\n",
    "システム:'''.replace('\\n','<NL>')\n",
    "\n",
    "token_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        do_sample=True,\n",
    "        max_new_tokens=128,\n",
    "        temperature=0.01,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0][token_ids.size(1):])\n",
    "output = output.replace(\"<NL>\", \"\\n\")\n",
    "print(output)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2affb9-346c-4d31-9051-a266fc7998c1",
   "metadata": {},
   "source": [
    "### モジュール読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0221f42-b4de-4c97-be49-c95915a2b945",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import sagemaker, boto3, json\n",
    "# from sagemaker.session import Session\n",
    "\n",
    "# sagemaker_session = Session()\n",
    "# aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "# aws_region = boto3.Session().region_name\n",
    "# sess = sagemaker.Session()\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import gc\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "import json\n",
    "region = boto3.session.Session().region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "sm = boto3.client('sagemaker')\n",
    "smr = boto3.client('sagemaker-runtime')\n",
    "endpoint_inservice_waiter = sm.get_waiter('endpoint_in_service')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133d50ea-0893-4c69-8eab-3fce415a97e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### モデルのダウンロード\n",
    "SageMaker で機械学習モデルをホスティングする際は、一般的にはモデルや推論コードなどを tar.gz の形に固めます。\n",
    "tokenizer と model を `from_pretrained` メソッドを利用してモデルをインターネットからロードして、そのままファイルをディレクトリに出力します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85daa4e6-6ec3-4ac0-ad39-10ac311390b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 既存のディレクトリがある場合のときのため削除\n",
    "model_dir = './inference'\n",
    "!rm -rf {model_dir}\n",
    "!mkdir -p {model_dir}'/code'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9a7060-a17b-462e-ad9f-63d6be60d841",
   "metadata": {},
   "source": [
    "### tokenizer の取得と保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b2b863e-1b06-428f-9a92-4538738445a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 284/284 [00:00<00:00, 43.2kB/s]\n",
      "Downloading spiece.model: 100%|██████████| 786k/786k [00:00<00:00, 89.4MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 160 ms, sys: 11.9 ms, total: 172 ms\n",
      "Wall time: 574 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./inference/tokenizer_config.json',\n",
       " './inference/special_tokens_map.json',\n",
       " './inference/spiece.model',\n",
       " './inference/added_tokens.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'rinna/japanese-gpt-neox-3.6b-instruction-ppo', \n",
    "    use_fast=False\n",
    ")\n",
    "tokenizer.save_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f07896-41b4-4692-aadb-03674198cd74",
   "metadata": {},
   "source": [
    "### モデルの取得と保存\n",
    "\n",
    "以下のセルは 10GB 以上のモデルを DL して保存するため 5 分ほど時間がかかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d32c3ea0-8b4b-4cd2-90aa-39dde6e9d5ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 534/534 [00:00<00:00, 90.3kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 7.40G/7.40G [00:19<00:00, 386MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42 s, sys: 29.6 s, total: 1min 11s\n",
      "Wall time: 4min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'rinna/japanese-gpt-neox-3.6b-instruction-ppo'\n",
    ")\n",
    "model.save_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8cc985-4f72-4d75-8a65-89e6e6e547b0",
   "metadata": {},
   "source": [
    "モデルは SageMaker で動かすのでメモリから開放します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3b1281d-46b3-436f-b4cf-231bd72464a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "259"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model\n",
    "del tokenizer\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132c5a3b-edce-49bc-a527-da0d59bd23d0",
   "metadata": {},
   "source": [
    "### 推論コードの作成\n",
    "先程実行したコードをもとに記述していきます。\n",
    "まずは必要なモジュールを記述した requirements.txt を用意します。\n",
    "今回は [deep-learning-containers](https://github.com/aws/deep-learning-containers)の HuggingFace のコンテナを使います。\n",
    "einops と Sentence Piece が不足しているので requirements.txt に記載します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d27c4aaf-de3f-4116-a2a2-1a01ef7053c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing inference/code/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile inference/code/requirements.txt\n",
    "einops\n",
    "SentencePiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd911771-218f-46b8-ad0d-bd28d25af5a4",
   "metadata": {},
   "source": [
    "先述のコードを SageMaker Inference 向けに改変します。\n",
    "\n",
    "1. `model_fn` でモデルを読み込みます。先程は huggingface のモデルを直接ロードしましたが、model_dir に展開されたモデルを読み込みます。\n",
    "2. `input_fn` で前処理を行います。\n",
    "  - json 形式のみを受け付け他の形式は弾くようにします。\n",
    "  - json 文字列を dict 形式に変換して返します。\n",
    "3. `predict_fn` で推論します。\n",
    "  - リクエストされたテキストを token 化します。\n",
    "  - パラメータを展開します。\n",
    "  - 推論（生成）します。\n",
    "  - 生成結果をテキストにして返します。\n",
    "4. `output_fn` で結果を json 形式にして返します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76735b8b-367f-4b81-91b4-b448f312d545",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing inference/code/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile inference/code/inference.py\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import json\n",
    "\n",
    "DEVICE = 'cuda:0'\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_dir, \n",
    "        use_fast=False\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_dir\n",
    "    ).to(DEVICE)\n",
    "    return {'tokenizer':tokenizer,'model':model}\n",
    "\n",
    "def input_fn(data, content_type):\n",
    "    if content_type == 'application/json':\n",
    "        data = json.loads(data)\n",
    "    else:\n",
    "        raise TypeError('content_type is only allowed application/json')\n",
    "    return data\n",
    "\n",
    "def predict_fn(data, model):\n",
    "    prompt = data['prompt']\n",
    "    token_ids = model['tokenizer'].encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    do_sample = data['do_sample']\n",
    "    max_new_tokens = data['max_new_tokens']\n",
    "    temperature = data['temperature']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output_ids = model['model'].generate(\n",
    "            token_ids.to(DEVICE),\n",
    "            do_sample=do_sample,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            pad_token_id=model['tokenizer'].pad_token_id,\n",
    "            bos_token_id=model['tokenizer'].bos_token_id,\n",
    "            eos_token_id=model['tokenizer'].eos_token_id\n",
    "        )\n",
    "    output = model['tokenizer'].decode(output_ids.tolist()[0][token_ids.size(1):])\n",
    "    output = output.replace(\"<NL>\", \"\\n\")\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def output_fn(data, accept_type):\n",
    "    if accept_type == 'application/json':\n",
    "        data = json.dumps({'result' : data})\n",
    "    else:\n",
    "        raise TypeError('content_type is only allowed application/json')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2bb788-04fb-468f-b136-86d9eff5268b",
   "metadata": {},
   "source": [
    "## モデルアーティファクトの作成と S3 アップロード\n",
    "アーティファクト(推論コード + モデル)を tar.gz に固めます。時間がかかるので `pigz` で並列処理を行います。\n",
    "ml.g5.2xlarge, ml.m5.2xlarge で 10 分ほどかかります。\n",
    "\n",
    "※ SageMaker Studio のカーネルには pigz が入っていないので、下記 apt のセルを実行してください。SageMaker Notebooks の場合は不要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "386020c8-8381-429e-9ccf-fbf79c663711",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 http://archive.ubuntu.com/ubuntu focal InRelease [265 kB]\n",
      "Get:2 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\u001b[33m\u001b[33m\u001b[33m\n",
      "Get:4 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [37.5 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
      "Get:6 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1070 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu focal/restricted amd64 Packages [33.4 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu focal/universe amd64 Packages [11.3 MB]\n",
      "Get:9 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2866 kB][33m\u001b[33m\u001b[33m\n",
      "Get:10 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [2529 kB]3m\n",
      "Get:11 http://archive.ubuntu.com/ubuntu focal/main amd64 Packages [1275 kB]    \u001b[0m\u001b[33m\u001b[33m\n",
      "Get:12 http://archive.ubuntu.com/ubuntu focal/multiverse amd64 Packages [177 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3346 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [40.2 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1369 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [2670 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [55.2 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [28.6 kB]\n",
      "Fetched 27.4 MB in 2s (11.1 MB/s)33m                      \u001b[0m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "93 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  pigz\n",
      "0 upgraded, 1 newly installed, 0 to remove and 93 not upgraded.\n",
      "Need to get 57.4 kB of archives.\n",
      "After this operation, 259 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 pigz amd64 2.4-1 [57.4 kB]\n",
      "Fetched 57.4 kB in 0s (181 kB/s)\u001b[0m\u001b[33m\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package pigz.\n",
      "(Reading database ... 40690 files and directories currently installed.)\n",
      "Preparing to unpack .../archives/pigz_2.4-1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 20%]\u001b[49m\u001b[39m [###########...............................................] \u001b8Unpacking pigz (2.4-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 40%]\u001b[49m\u001b[39m [#######################...................................] \u001b8Setting up pigz (2.4-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 60%]\u001b[49m\u001b[39m [##################################........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 80%]\u001b[49m\u001b[39m [##############################################............] \u001b8\n",
      "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[J"
     ]
    }
   ],
   "source": [
    "!apt update -y\n",
    "!apt install pigz -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93917a56-2f5a-4576-9f47-17f60088df74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'model.tar.gz': No such file or directory\n",
      "/root/apj-generative-ai-l300-bootcamp-main/Day2/inference\n",
      "./\n",
      "./pytorch_model-00002-of-00002.bin\n",
      "./spiece.model\n",
      "./tokenizer_config.json\n",
      "./config.json\n",
      "./pytorch_model.bin.index.json\n",
      "./generation_config.json\n",
      "./special_tokens_map.json\n",
      "./code/\n",
      "./code/requirements.txt\n",
      "./code/inference.py\n",
      "./pytorch_model-00001-of-00002.bin\n",
      "/root/apj-generative-ai-l300-bootcamp-main/Day2\n",
      "CPU times: user 8.69 s, sys: 1.04 s, total: 9.73 s\n",
      "Wall time: 9min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!rm model.tar.gz\n",
    "%cd {model_dir}\n",
    "!tar  cv ./ | pigz -p 8 > ../model.tar.gz # 8 並列でアーカイブ\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e26b13-0fd7-4289-bf9b-8d074ef6cc5b",
   "metadata": {},
   "source": [
    "アーティファクトを S3 にアップロードします。60 秒程度で完了します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25474740-4439-4c5d-9ecc-e6ffe73c68a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-809078683005/japanese-gpt-neox-3.6b-instruction-ppo/model.tar.gz\n",
      "CPU times: user 46.7 s, sys: 48.2 s, total: 1min 34s\n",
      "Wall time: 33.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_s3_uri = sagemaker.session.Session().upload_data(\n",
    "    'model.tar.gz',\n",
    "    key_prefix='japanese-gpt-neox-3.6b-instruction-ppo'\n",
    ")\n",
    "print(model_s3_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1530e67a-61a1-42e4-b177-90a7ed6f8423",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SageMaker で Hosting する\n",
    "g5.2xlarge インスタンス(NVIDIA A10G Tensor Core GPU 搭載 VRAM 24GB, RAM 32GB) の場合レスポンスに 6 秒程度で済むため、リアルタイム推論エンドポイントを立てます。\n",
    "[g5.2xlarge の料金はこちら](https://aws.amazon.com/sagemaker/pricing/?nc1=h_ls)で確認してください。\n",
    "\n",
    "<!--リアルタイム推論エンドポイントを立てて推論するにあたって、SageMaker Python SDK を用いる場合と Boto3 を用いる場合の 2 パターンを紹介します。-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e732d85-bb01-4af1-bdb0-3eb2b5377130",
   "metadata": {},
   "source": [
    "### SageMaker Python SDKを用いる場合\n",
    "#### Hosting\n",
    "使用している API の詳細は以下を確認してください。\n",
    "Amazon SageMaker Python SDK\n",
    "\n",
    "#### 定数の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c878c6c3-40a0-4ae8-bdbe-1d461075cfc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = 'japanese-gpt-neox-3-6b-instruction-ppo'\n",
    "endpoint_config_name = model_name + '-Config'\n",
    "endpoint_name = model_name + '-Endpoint'\n",
    "instance_type = 'ml.g5.2xlarge'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71f58cb-ecd8-434e-83fa-d3c35fb235a3",
   "metadata": {},
   "source": [
    "#### 使用するコンテナイメージの URI を取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b66dfd6e-b356-42fd-a5b9-953d2c40c348",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework='huggingface',\n",
    "    region=region,\n",
    "    version='4.26',\n",
    "    image_scope='inference',\n",
    "    base_framework_version='pytorch1.13',\n",
    "    instance_type = instance_type\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b6b79b-c6c6-4885-84fb-ce89ee48c8eb",
   "metadata": {},
   "source": [
    "#### モデルの定義\n",
    "先程 S3 にアップロードしたアーティファクトの tar.gz の URI と、コンテナイメージの URI, ロールを設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ed478f1-1217-4abd-91bc-b8c32f05e570",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_s3_uri' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3f3fbb4b06da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m huggingface_model = HuggingFaceModel(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_s3_uri\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mrole\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrole\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mimage_uri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_uri\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_s3_uri' is not defined"
     ]
    }
   ],
   "source": [
    "huggingface_model = HuggingFaceModel(\n",
    "    model_data = model_s3_uri,\n",
    "    role = role,\n",
    "    image_uri = image_uri\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fabf37-c8c8-4d1b-b061-027c676db59a",
   "metadata": {},
   "source": [
    "#### デプロイ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "982d95af-5823-4778-9e25-856d021259bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "05c80bc8-8d61-4fad-a73e-39d9714ba3e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'japanese-gpt-neox-3-6b-instruction-ppo-Endpoint'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_name = predictor.endpoint_name\n",
    "endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "16f41304-653e-4a03-b3e1-e0feb326c92c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'endpoint_name' (str)\n"
     ]
    }
   ],
   "source": [
    "%store endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ea4f73-db3e-4e22-ae5e-2904d5f482c7",
   "metadata": {},
   "source": [
    "### 推論\n",
    "#### promptについて\n",
    "[japanese-gpt-neox-3.6b-instruction-ppo#io-format](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-ppo#io-format) にある通り、以下の通りにすると良い結果が得られやすいです。\n",
    "\n",
    "- プロンプトはユーザーとシステムの会話形式で与える\n",
    "- 各発言は、以下形式に則る\n",
    "`{ユーザー, システム} : {発言}`\n",
    "- プロンプトの末尾は `システム:` で終了させる\n",
    "- 改行は `<NL>` を利用し、発言はすべて `<NL>` で区切る必要がある\n",
    "以下はプロンプトの例です。`<NL>` の埋め込みが大変なので、改行で書いて後で置換します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93cff521-1400-44c1-a1b2-eb2dedd5b95d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ユーザー: 世界自然遺産を列挙してください。<NL>システム: 膨大な数です。例えば国で絞ってください。<NL>ユーザー: イギリスでお願いします。<NL>システム:\n"
     ]
    }
   ],
   "source": [
    "prompt = '''ユーザー: 世界自然遺産を列挙してください。\n",
    "システム: 膨大な数です。例えば国で絞ってください。\n",
    "ユーザー: イギリスでお願いします。\n",
    "システム:'''.replace('\\n','<NL>')\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30544249-101a-4a9f-bb70-326df8db534d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 推論リクエスト\n",
    "model_fn の実行に時間がかかってしまい、エンドポイントが IN_SERVICE になっても、初回推論はしばらく動かないことがあります。\n",
    "CloudWatch Logs に以下のような表示がある場合はしばらく待てば使えるようになります。\n",
    "> `[WARN] pool-3-thread-1 com.amazonaws.ml.mms.metrics.MetricCollector - worker pid is not available yet.`\n",
    "\n",
    "モデルがロードされるまで 6 分程度かかるため、リトライを入れています。 実際の推論時間は 6 秒程度です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a0bb712-0cdb-41e9-be09-d0c0b83d57e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "イギリスには、スコットランド、ウェールズ、イングランド、北アイルランドの世界遺産があります。これらは、世界で最も古い自然遺産のいくつかです。</s>\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "request = {\n",
    "    'prompt' : prompt,\n",
    "    'max_new_tokens' : 256,\n",
    "    'do_sample' : True,\n",
    "    'temperature' : 0.01,\n",
    "}\n",
    "\n",
    "for i in range(10):\n",
    "    try:\n",
    "        output = predictor.predict(request)['result']\n",
    "        break\n",
    "    except:\n",
    "        sleep(60)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68dbd1e9-9c4b-49b5-a0dc-3cefd116f3ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NL>システム: システムは資料から抜粋して質問に答えます。資料にない内容は答えず、正直に「わかりません」と答えます。<NL><NL>Amazon Kendra は、機械学習 (ML) を利用する高精度で使いやすいエンタープライズ検索サービスです。デベロッパーはアプリケーションに検索機能を追加できます。これにより、その企業全体に散在する膨大な量のコンテンツ内に保存されている情報をエンドユーザーが見つけられるようになります。これには、マニュアル、調査報告書、よくある質問、人事 (HR) 関連ドキュメント、カスタマーサービスガイドのデータが含まれます。Amazon Simple Storage Service (S3)、Microsoft SharePoint、Salesforce、ServiceNow、RDS データベース、Microsoft OneDrive などの様々なシステムに存在している場合があります。質問が入力されると、このサービスは機械学習アルゴリズムを使用してその内容を理解し、質問の直接の回答であれ、ドキュメント全体であれ、最も適切な回答を返します。例えば、「企業クレジットカードのキャッシュバック率はどれくらいですか?」といった質問をすることができ、Amazon Kendra は関連するドキュメントにマッピングして具体的な回答 (「2% です」など) を返します。Kendra はサンプルコードを提供するため、ユーザーは迅速に使用を開始し、新規または既存のアプリケーションに極めて正確な検索を簡単に統合できます。<NL><NL>上記の資料に基づいて以下の質問について資料から抜粋して回答を生成します。資料にない内容は答えず「わかりません」と答えます。<NL>ユーザー: Amazon Kendraはどのデータソースから回答を返してくれますか？<NL>\n"
     ]
    }
   ],
   "source": [
    "question = \"Amazon Kendraはどのデータソースから回答を返してくれますか？\"\n",
    "#question = \"犬は何を食べますか？\"\n",
    "context = \"Amazon Kendra は、機械学習 (ML) を利用する高精度で使いやすいエンタープライズ検索サービスです。\" \\\n",
    "\"デベロッパーはアプリケーションに検索機能を追加できます。\" \\\n",
    "\"これにより、その企業全体に散在する膨大な量のコンテンツ内に保存されている情報をエンドユーザーが見つけられるようになります。\" \\\n",
    "\"これには、マニュアル、調査報告書、よくある質問、人事 (HR) 関連ドキュメント、カスタマーサービスガイドのデータが含まれます。\" \\\n",
    "\"Amazon Simple Storage Service (S3)、Microsoft SharePoint、Salesforce、ServiceNow、RDS データベース、Microsoft OneDrive などの様々なシステムに存在している場合があります。\" \\\n",
    "\"質問が入力されると、このサービスは機械学習アルゴリズムを使用してその内容を理解し、質問の直接の回答であれ、ドキュメント全体であれ、最も適切な回答を返します。\" \\\n",
    "\"例えば、「企業クレジットカードのキャッシュバック率はどれくらいですか?」といった質問をすることができ、Amazon Kendra は関連するドキュメントにマッピングして具体的な回答 (「2% です」など) を返します。\" \\\n",
    "\"Kendra はサンプルコードを提供するため、ユーザーは迅速に使用を開始し、新規または既存のアプリケーションに極めて正確な検索を簡単に統合できます。\"\n",
    "prompt = f\"\"\"\n",
    "システム: システムは資料から抜粋して質問に答えます。資料にない内容は答えず、正直に「わかりません」と答えます。\n",
    "\n",
    "{context}\n",
    "\n",
    "上記の資料に基づいて以下の質問について資料から抜粋して回答を生成します。資料にない内容は答えず「わかりません」と答えます。\n",
    "ユーザー: {question}\n",
    "\"\"\".replace(\"\\n\", \"<NL>\")\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34c4280e-7f40-4fab-903a-7ce9b9132b03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon Simple Storage Service (S3)、Microsoft SharePoint、Salesforce、RDS データベース、Microsoft OneDrive などの様々なシステムに存在している場合があります。回答は、機械学習アルゴリズムを使用して、関連するドキュメントにマッピングし、適切な回答を返します。</s>\n"
     ]
    }
   ],
   "source": [
    "request = {\n",
    "    'prompt' : prompt,\n",
    "    'max_new_tokens' : 256,\n",
    "    'do_sample' : True,\n",
    "    'temperature' : 0.7,\n",
    "    'repetition_penalty': 1.3,\n",
    "}\n",
    "output = predictor.predict(request)['result']\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80864705-fcba-49f7-9b61-cc0d51d5be40",
   "metadata": {},
   "source": [
    "#### エンドポイントの削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9a00b4-d7e7-42ef-8520-74dfd6657915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor.delete_model()\n",
    "# predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17ed3d4d-5c77-42d3-bf1d-dccc6be203b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model_id, model_version = \"huggingface-text2text-flan-t5-xl\", \"*\"\n",
    "#model_id, model_version = \"huggingface-textgeneration1-mpt-7b-instruct-bf16\", \"*\"\n",
    "model_id, model_version = \"huggingface-llm-falcon-7b-instruct-bf16\", \"*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1929e8a-7076-4119-a527-7b47126e67a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5. Query endpoint with Langchain and Kendra Index\n",
    "\n",
    "---\n",
    "Now we will use use the `KendraIndexRetriever` retriever class with Langchain to retrieve information from our Kendra Index that matches the query.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0396373c-afe2-4dea-9110-774882043544",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"IndexConfigurationSummaryItems\": [\n",
      "        {\n",
      "            \"Name\": \"demo-index\",\n",
      "            \"Id\": \"c9f7203c-bad2-45bf-a0ed-dbaf0e201e93\",\n",
      "            \"Edition\": \"DEVELOPER_EDITION\",\n",
      "            \"CreatedAt\": 1678180374.447,\n",
      "            \"UpdatedAt\": 1687854347.072,\n",
      "            \"Status\": \"ACTIVE\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!aws kendra list-indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fb18f64-b235-4959-b6d5-d077b3028a44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# UPDATE THE FOLLOWING WITH THE OUTPUTS FROM THE CLOUDFORMATION DEPLOYMENT\n",
    "kendra_index_id=\"c9f7203c-bad2-45bf-a0ed-dbaf0e201e93\"\n",
    "region=\"us-east-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07852209-3729-4e55-9a6a-86b3c1901a11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'japanese-gpt-neox-3-6b-instruction-ppo-Endpoint'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%store -r endpoint_name\n",
    "endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c2df39c-9a53-4204-93da-fa2cbfa8bd18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFacePredictor\n",
    "predictor = HuggingFacePredictor(endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "728b0d52-1439-4e81-b762-f3d858eb3069",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "newline, bold, unbold = \"\\n\", \"\\033[1m\", \"\\033[0m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "af4cc080-98b7-4bb8-990b-1b3bb5b8a1b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:  \n",
      "システム: システムは資料から抜粋して質問に答えます。資料にない内容には答えず、正直に「わかりません」と答えます。\n",
      "\n",
      "Document Title: wellarchitected-reliability-pillar \n",
      "Document Excerpt: \n",
      "• REL02-BP04 多対多メッシュよりもハブアンドスポークトポロジを優先する (p. 36) • REL02-BP05 接続されているすべてのプライベートアドレス空間において、重複しないプライベート IP アドレス範囲を指定する (p. 38) REL02-BP01 ワークロードのパブリックエンドポイン トに高可用性ネットワーク接続を使用する ワークロードのパブリックエンドポイントに高可用性ネットワーク接続を構築すると、接続の喪失による ダウンタイムを低減し、ワークロードの可用性と SLA を向上できます。 これを実現するには、可用性の高 い DNS、コンテンツ配信ネットワーク、API ゲートウェイ、負荷分散、またはリバースプロキシを使用し ます。 期待される成果: パブリックエンドポイントの高可用性ネットワーク接続を計画、構築、運用化すること が重要です。 接続が失われたためにワークロードにアクセスできなくなった場合、ワークロードが実行中 28 http://aws.amazon.com/codedeploy/ http://aws.amazon.com/cloudtrail/ http://aws.amazon.com/cloudwatch/ http://aws.amazon.com/eventbridge/ http://aws.amazon.com/devops-guru/ http://aws.amazon.com/config/ http://aws.amazon.com/premiumsupport/technology/trusted-advisor/ http://aws.amazon.com/cdk/ http://aws.amazon.com/systems-manager/\n",
      "\n",
      "\n",
      "Document Title: 可用性ニーズの理解 - 信頼性の柱 \n",
      "Document Excerpt: \n",
      "AWS のソリューションアーキテクト (SA) は、お客様の可用性目標に対する適切な設計を支援します。 設計プロセスの初期段階から AWS を導入していただくことで、私たちが可用性目標の達成を支援する能力も向上します。 可用性の計画は、実際のワークロードが始動する直前にだけ立てるものではありません。 また、運用上の経験を積み、現実世界の出来事から学び、さまざまなタイプの障害に耐えながら、設計を改良し続けることも行います。 そうすることで、適切な作業を行って実際の運用を改善できます。 ワークロードに求められる可用性のニーズは、ビジネスのニーズと重要度に合わせる必要があります。 まず、RTO、RPO、および可用性を明確にしてビジネスにとって何が重要なのかのフレームワークを明確にすることで、各ワークロードを評価できます。 このようなアプローチでは、ワークロードを実際に運用する人がフレームワークに精通し、そのワークロードがビジネスニーズに与える影響を理解している必要があります。 ブラウザで JavaScript が無効になっているか、使用できません。 AWS ドキュメントを使用するには、JavaScript を有効にする必要があります。\n",
      "\n",
      "\n",
      "Document Title: 可用性ニーズの理解 - 信頼性の柱 \n",
      "Document Excerpt: \n",
      "可用性の計画は、実際のワークロードが始動する直前にだけ立てるものではありません。 また、運用上の経験を積み、現実世界の出来事から学び、さまざまなタイプの障害に耐えながら、設計を改良し続けることも行います。 そうすることで、適切な作業を行って実際の運用を改善できます。 ワークロードに求められる可用性のニーズは、ビジネスのニーズと重要度に合わせる必要があります。 まず、RTO、RPO、および可用性を明確にしてビジネスにとって何が重要なのかのフレームワークを明確にすることで、各ワークロードを評価できます。 このようなアプローチでは、ワークロードを実際に運用する人がフレームワークに精通し、そのワークロードがビジネスニーズに与える影響を理解している必要があります。 ブラウザで JavaScript が無効になっているか、使用できません。 AWS ドキュメントを使用するには、JavaScript を有効にする必要があります。 手順については、使用するブラウザのヘルプページを参照してください。 ドキュメントの表記規則 ディザスタリカバリ (DR) 目標\n",
      "\n",
      "\n",
      "上記の資料に基づいて以下の質問について資料から抜粋して回答を生成します。資料にない内容には答えず「わかりません」と答えます。\n",
      "ユーザー: ワークロードの可用性を上げたい\n",
      "システム:\n",
      "\n",
      "\u001b[1mAnswer\u001b[0m: 重要なことは、ワークロードの可用性計画を立案し、実行することです。これにより、ワークロードを運用する人々が、障害に迅速かつ簡単に対応し、ビジネスの重要な資産を保護できます。 これにより、可用性が向上し、重要なサービスの中断を最小限に抑えることができます。 これにより、重要なサービスの障害によるダウンタイムを最小限に抑え、重要なビジネス資産を保護できます。 重要なことは、計画を立て、実行し、継続的な改善を行うことです。</s>\n",
      "\n",
      "\u001b[1mSources:\u001b[0m\n",
      "\n",
      "wellarchitected-reliability-pillar\n",
      "https://docs.aws.amazon.com/ja_jp/wellarchitected/latest/reliability-pillar/wellarchitected-reliability-pillar.pdf\n",
      "• REL02-BP04 多対多メッシュよりもハブアンドスポークトポロジを優先する (p. 36) • REL02-BP05 接続されているすべてのプライベートアドレス空間において、重複しないプライベート IP アドレス範囲を指定する (p. 38) REL02-BP01 ワークロードのパブリックエンドポイン トに高可用性ネットワーク接続を使用する ワークロードのパブリックエンドポイントに高可用性ネットワーク接続を構築すると、接続の喪失による ダウンタイムを低減し、ワークロードの可用性と SLA を向上できます。 これを実現するには、可用性の高 い DNS、コンテンツ配信ネットワーク、API ゲートウェイ、負荷分散、またはリバースプロキシを使用し ます。 期待される成果: パブリックエンドポイントの高可用性ネットワーク接続を計画、構築、運用化すること が重要です。 接続が失われたためにワークロードにアクセスできなくなった場合、ワークロードが実行中 28 http://aws.amazon.com/codedeploy/ http://aws.amazon.com/cloudtrail/ http://aws.amazon.com/cloudwatch/ http://aws.amazon.com/eventbridge/ http://aws.amazon.com/devops-guru/ http://aws.amazon.com/config/ http://aws.amazon.com/premiumsupport/technology/trusted-advisor/ http://aws.amazon.com/cdk/ http://aws.amazon.com/systems-manager/\n",
      "\n",
      "\n",
      "可用性ニーズの理解 - 信頼性の柱\n",
      "https://docs.aws.amazon.com/ja_jp/wellarchitected/latest/reliability-pillar/understanding-availability-needs.html\n",
      "AWS のソリューションアーキテクト (SA) は、お客様の可用性目標に対する適切な設計を支援します。 設計プロセスの初期段階から AWS を導入していただくことで、私たちが可用性目標の達成を支援する能力も向上します。 可用性の計画は、実際のワークロードが始動する直前にだけ立てるものではありません。 また、運用上の経験を積み、現実世界の出来事から学び、さまざまなタイプの障害に耐えながら、設計を改良し続けることも行います。 そうすることで、適切な作業を行って実際の運用を改善できます。 ワークロードに求められる可用性のニーズは、ビジネスのニーズと重要度に合わせる必要があります。 まず、RTO、RPO、および可用性を明確にしてビジネスにとって何が重要なのかのフレームワークを明確にすることで、各ワークロードを評価できます。 このようなアプローチでは、ワークロードを実際に運用する人がフレームワークに精通し、そのワークロードがビジネスニーズに与える影響を理解している必要があります。 ブラウザで JavaScript が無効になっているか、使用できません。 AWS ドキュメントを使用するには、JavaScript を有効にする必要があります。\n",
      "\n",
      "\n",
      "可用性ニーズの理解 - 信頼性の柱\n",
      "https://docs.aws.amazon.com/ja_jp/wellarchitected/latest/reliability-pillar/understanding-availability-needs.html\n",
      "可用性の計画は、実際のワークロードが始動する直前にだけ立てるものではありません。 また、運用上の経験を積み、現実世界の出来事から学び、さまざまなタイプの障害に耐えながら、設計を改良し続けることも行います。 そうすることで、適切な作業を行って実際の運用を改善できます。 ワークロードに求められる可用性のニーズは、ビジネスのニーズと重要度に合わせる必要があります。 まず、RTO、RPO、および可用性を明確にしてビジネスにとって何が重要なのかのフレームワークを明確にすることで、各ワークロードを評価できます。 このようなアプローチでは、ワークロードを実際に運用する人がフレームワークに精通し、そのワークロードがビジネスニーズに与える影響を理解している必要があります。 ブラウザで JavaScript が無効になっているか、使用できません。 AWS ドキュメントを使用するには、JavaScript を有効にする必要があります。 手順については、使用するブラウザのヘルプページを参照してください。 ドキュメントの表記規則 ディザスタリカバリ (DR) 目標\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from aws_langchain.kendra_index_retriever import KendraIndexRetriever\n",
    "from langchain.retrievers import AmazonKendraRetriever\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "import json\n",
    "\n",
    "class RinnaContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: dict) -> bytes:\n",
    "        input_str = json.dumps(\n",
    "            {\n",
    "                \"instruction\": \"\",\n",
    "                #\"input\": prompt.replace(\"\\n\", \"<NL>\"),\n",
    "                \"prompt\": prompt.replace(\"\\n\", \"<NL>\"),\n",
    "                **model_kwargs,\n",
    "            }\n",
    "        )\n",
    "        print(\"prompt: \", prompt)\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        #return response_json.replace(\"<NL>\", \"\\n\")\n",
    "        #print(response_json)\n",
    "        return response_json[\"result\"]\n",
    "\n",
    "#     def transform_input(self, prompt: str, model_kwargs: dict) -> bytes:\n",
    "#         input_str = json.dumps({\"text_inputs\": prompt, **model_kwargs})\n",
    "#         return input_str.encode('utf-8')\n",
    "\n",
    "#     def transform_output(self, output: bytes) -> str:\n",
    "#         response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "#         return response_json[\"generated_texts\"][0]\n",
    "\n",
    "content_handler = RinnaContentHandler()\n",
    "llm=SagemakerEndpoint(\n",
    "        endpoint_name=endpoint_name,\n",
    "        region_name=\"us-east-1\", \n",
    "        model_kwargs={\n",
    "            \"temperature\":1e-10,\n",
    "            \"max_new_tokens\": 256,\n",
    "            \"do_sample\": True,\n",
    "            \"temperature\" : 0.7,\n",
    "            \"repetition_penalty\": 1.3,\n",
    "        },\n",
    "        content_handler=content_handler\n",
    "    )\n",
    "\n",
    "#retriever = KendraIndexRetriever(kendraindex=kendra_index_id,\n",
    "retriever = AmazonKendraRetriever(\n",
    "        index_id=kendra_index_id,\n",
    "        region_name=region,\n",
    "        top_k=3,\n",
    "        credentials_profile_name=None,\n",
    "        attribute_filter={\n",
    "            \"EqualsTo\": {      \n",
    "                \"Key\": \"_language_code\",\n",
    "                \"Value\": {\n",
    "                    \"StringValue\": \"ja\"\n",
    "                    }\n",
    "                }\n",
    "        }\n",
    "#        return_source_documents=True\n",
    "    )\n",
    "\n",
    "# prompt_template = \"\"\"\n",
    "# The following is a friendly conversation between a human and an AI.\n",
    "# The AI is talkative and provides lots of specific details from its context.\n",
    "# If the AI does not know the answer to a question, it truthfully says it\n",
    "# does not know.\n",
    "# {context}\n",
    "# Instruction: Based on the above documents, provide a detailed answer for, {question} Answer \"don't know\" if not present in the document. Solution:\n",
    "# \"\"\"\n",
    "prompt_template = \"\"\"\n",
    "システム: システムは資料から抜粋して質問に答えます。資料にない内容には答えず、正直に「わかりません」と答えます。\n",
    "\n",
    "{context}\n",
    "\n",
    "上記の資料に基づいて以下の質問について資料から抜粋して回答を生成します。資料にない内容には答えず「わかりません」と答えます。\n",
    "ユーザー: {question}\n",
    "システム:\n",
    "\"\"\"#.replace(\"\\n\", \"<NL>\")\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs=chain_type_kwargs,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "result = qa(\"ワークロードの可用性を上げたい\")\n",
    "#result = qa(\"犬かわいすぎ\")\n",
    "print(f'{bold}Answer{unbold}: {result[\"result\"]}\\n\\n{bold}Sources:{unbold}')\n",
    "\n",
    "for doc in result['source_documents']:\n",
    "    print(f'''\\n{doc.metadata[\"title\"]}\\n{doc.metadata[\"source\"]}\\n{doc.metadata[\"excerpt\"]}\\n''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b10281-27da-4744-b265-00ab3066df81",
   "metadata": {},
   "source": [
    "### Example Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5c20f1-9360-4637-9649-dede0feaabc8",
   "metadata": {},
   "source": [
    "The quality of the response you will recieve is going to depend on the model you have deployed. Try deploying a different model endpoint and comparing the results you recieve from the same queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d3a15a-06f2-4b70-997e-558b3810a1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa(\"ワークロードの可用性を上げたい\")\n",
    "print(f'{bold}Answer{unbold}: {result[\"result\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1ddc3a1b-2910-4729-807d-38031264dd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:  \n",
      "システム: システムは資料から抜粋して質問に答えます。資料にない内容には答えず、正直に「わかりません」と答えます。\n",
      "\n",
      "Document Title: wellarchitected-reliability-pillar \n",
      "Document Excerpt: \n",
      "関連動画: • AWS の静的安定性: AWS re:Invent 2019: Introducing The Amazon Builders’ Library (DOP328) REL11-BP06 イベントが可用性に影響する場合に通知 を送信する 重大なイベントが検出されると、イベントによって引き起こされた問題が自動的に解決された場合でも、 通知が送信されます。 自動ヒーリング機能により、ワークロードの信頼性を高めることができます。 ただし、対処する必要のあ る根本的な問題もあいまいになる可能性があります。 根本原因の問題を解決できるように、自動ヒーリ ングによって対処されたものを含む問題のパターンを検出できるように、適切なモニタリングとイベン トを実装します。 Amazon CloudWatch アラームは、発生した障害に基づいてトリガーできます。 また、 実行された自動ヒーリングアクションに基づいてトリガーすることもできます。 CloudWatch アラーム は、Amazon SNS 統合を使用して、E メールを送信するか、サードパーティのインシデント追跡システム\n",
      "\n",
      "\n",
      "Document Title: wellarchitected-reliability-pillar \n",
      "Document Excerpt: \n",
      "が高いため、許可すべきではありません。 リソース 関連するドキュメント: • 災害対策プランでの依存関係の最小化 • The Amazon Builders' Library: アベイラビリティーゾーンを使用した静的安定性 関連動画: • AWS の静的安定性: AWS re:Invent 2019: Introducing The Amazon Builders’ Library (DOP328) REL11-BP06 イベントが可用性に影響する場合に通知 を送信する 重大なイベントが検出されると、イベントによって引き起こされた問題が自動的に解決された場合でも、 通知が送信されます。 自動ヒーリング機能により、ワークロードの信頼性を高めることができます。 ただし、対処する必要のあ る根本的な問題もあいまいになる可能性があります。 根本原因の問題を解決できるように、自動ヒーリ ングによって対処されたものを含む問題のパターンを検出できるように、適切なモニタリングとイベン トを実装します。 Amazon CloudWatch アラームは、発生した障害に基づいてトリガーできます。\n",
      "\n",
      "\n",
      "Document Title: wellarchitected-reliability-pillar \n",
      "Document Excerpt: \n",
      "手動または自動のどちらでも、標 準的なアクティビティを実行するにはランブックを使用します。 例えば、ワークロードのデプロイ、ワー クロードへのパッチの適用、DNS の変更などがあります。 例えば、デプロイ中のロールバックの安全性を 確保するためのプロセスを導入します。 顧客側の中断なし でデプロイをロールバックできるようにすることは、サービスの信頼性を高める上で重要です。 ランブックの手順については、有効で効果的な手動プロセスから開始し、それをコードで実装して、適切 な場合は自動実行をトリガーします。 高度に自動化された高機能のワークロードの場合でも、ランブックは 本番環境で実行したり、 厳格なレ ポートや監査の要件を満たしたりするのに役立ちます。 プレイブックは特定のインシデントに対応するために用いられ、ランブックは特定の結果を達成するため に使用されます。 多くの場合、ランブックは日常的なアクティビティ用で、プレイブックは非日常的なイ\n",
      "\n",
      "\n",
      "上記の資料に基づいて以下の質問について資料から抜粋して回答を生成します。資料にない内容には答えず「わかりません」と答えます。\n",
      "ユーザー: AWSの信頼性を高める機能についてを教えてください。\n",
      "\n",
      "\u001b[1mAnswer\u001b[0m: AWSは、重大な障害の発生を検知し、トリガーされたイベントによって引き起こされた問題を自動的に修復します。 これにより、ワークロードの信頼性が向上します。 これにより、顧客側はより迅速な問題の修復ができ、サービスの可用性が高まります。 また、信頼性を高めることによって、顧客がサービスを利用することに対する安心感が増します。</s>\n"
     ]
    }
   ],
   "source": [
    "result = qa(\"AWSの信頼性を高める機能についてを教えてください。\")\n",
    "print(f'{bold}Answer{unbold}: {result[\"result\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e9c252-6851-4386-b042-653316f70075",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 6. [OPTIONAL] Installing Streamlit application and running a WebUI for a chatbot\n",
    "\n",
    "---\n",
    "This sections provides instructions on how to run a streamlet application within sagemaker studio and accessing it using jupyter proxy. The commands and instructions below need to be run inside a **SageMaker System Terminal**.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8dacab-21f1-416d-a611-f40d857ddd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/kmotohas/amazon-kendra-langchain-extensions.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2533b228-0765-4485-971d-e1ef4f1d3cc3",
   "metadata": {},
   "source": [
    "1. Launch a new SageMaker System Terminal \n",
    "   1. From the SageMaker Studio Home screen select `Open Launcher`\n",
    "   2. From the Launcher panel under `Utilities and files` select `System terminal`\n",
    "\n",
    "5. Set your environment variables\n",
    "```\n",
    "export AWS_REGION=\"<YOUR_AWS_REGION>\"\n",
    "export KENDRA_INDEX_ID=\"<YOUR_KENDRA_INDEX_ID>\"\n",
    "export RINNA_ENDPOINT=\"<YOUR_SAGEMAKER_ENDPOINT_FOR_RINNA>\"\n",
    "```\n",
    "6. Run the streamlit application\n",
    "```\n",
    "cd ./amazon-kendra-langchain-extensions/kendra_retriever_samples\n",
    "streamlit run app.py rinna\n",
    "```\n",
    "7. This will output something similar to the below, you need to take note of the port (in this case 8501)\n",
    "```\n",
    "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n",
    "\n",
    "\n",
    "  You can now view your Streamlit app in your browser.\n",
    "\n",
    "  Network URL: http://169.255.255.2:8501\n",
    "  External URL: http://18.213.200.192:8501\n",
    "```\n",
    "8. Copy the current URL of the SageMaker Studio which should have the form:\n",
    "```\n",
    "https://<YOUR_STUDIO_DOMAIN>.studio.<AWS_REGION>.sagemaker.aws/jupyter/default/lab/workspaces/auto-Z/tree/kendra_rag_demo.ipynb\n",
    "```\n",
    "9. Delete everything from `lab/` onwards and replace it with `proxy/<PORT>/`\n",
    "   1. DON'T FORGET THE END `/`\n",
    "```\n",
    "https://<YOUR_STUDIO_DOMAIN>.studio.<AWS_REGION>.sagemaker.aws/jupyter/default/proxy/8501/\n",
    "```\n",
    "10. Paste the new address into the browser and you will now be able to access your chatbot UI which uses Langchain and Kendra. Each response will list the sources from Kendra it used for its answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d37967-38b0-47b2-b36a-1938a1a239d3",
   "metadata": {},
   "source": [
    "### 7. Clean up the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3eb3d629-4778-4b72-b65e-2597ffa21805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the SageMaker endpoint\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907d23e2-283e-47fe-90b9-da7a0ef12872",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.m5.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.13 Python 3.9 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.13-cpu-py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
